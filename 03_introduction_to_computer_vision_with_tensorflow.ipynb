{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNspQkcAssuv/iICA4naZoW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debanandasen/Tensorflow/blob/main/03_introduction_to_computer_vision_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Convolutional Neural Networks and Computer Vision with Tensorflow.\n",
        "\n",
        "### Computer vision is the practice of writing algorithoms which can dicsover patterns in visual data."
      ],
      "metadata": {
        "id": "_zTHkftJB4w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the Data\n",
        "\n",
        "Images we are working with are from Food101 dataset ,modified only to use two classes"
      ],
      "metadata": {
        "id": "hXkrS1ObCnhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\n",
        "# Unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"pizza_steak.zip\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "zck9HOJGCpAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse and understand the Data (visualizing the data)"
      ],
      "metadata": {
        "id": "ELcvPxjxGRmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls pizza_steak"
      ],
      "metadata": {
        "id": "6UgfBZLdGSTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls pizza_steak/train/steak"
      ],
      "metadata": {
        "id": "2SC8C2aAGcJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirpath,dirnames,filenames in os.walk(\"pizza_steak\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "VydvIFW0GkG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of images in a file\n",
        "num_steak_images_train = len(os.listdir(\"pizza_steak/train/steak\"))\n",
        "print(f\"There are {(num_steak_images_train)} images\")\n"
      ],
      "metadata": {
        "id": "fz-KuVENH4rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the classnmes programmatically\n",
        "import pathlib\n",
        "import numpy as np\n",
        "data_dir = pathlib.Path(\"pizza_steak/train\")\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob(\"*\")]))\n",
        "class_names"
      ],
      "metadata": {
        "id": "_HzUbKxaJb9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize our images\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "\n",
        "def view_random_image(target_dir,target_class):\n",
        "  #Setup the traget directory ( We'll view images from here)\n",
        "  target_folder = target_dir+target_class\n",
        "\n",
        "  # Get a random image path\n",
        "  random_image = random.sample(os.listdir(target_folder),1)\n",
        "  print(random_image)\n",
        "\n",
        "  # Read in the image and plot it using matplotlib\n",
        "  img = mpimg.imread(target_folder + \"/\" + random_image[0])\n",
        "  plt.imshow(img)\n",
        "  plt.title(target_class)\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "  print(f\"Image Shape: {img.shape}\")\n",
        "  return img\n"
      ],
      "metadata": {
        "id": "nRL3l_E4OdO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View a random image from the training dataset\n",
        "img = view_random_image(target_dir=\"pizza_steak/train/\",\n",
        "                        target_class=\"steak\")"
      ],
      "metadata": {
        "id": "vl9mPupeRB-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view the img (array/tensor)\n",
        "img"
      ],
      "metadata": {
        "id": "JfyW5sIfRhe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape # returns (width,height ,colours channels)"
      ],
      "metadata": {
        "id": "cki7rwHQDylR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize or scale the image\n",
        "img/255"
      ],
      "metadata": {
        "id": "f3LQh9WLD3Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.src.layers.serialization import activation\n",
        "from keras.src.layers.pooling.max_pooling2d import MaxPool2D\n",
        "from keras.src import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Set the seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Pre-processing the data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Setup the test and train directories\n",
        "\n",
        "train_dir = \"pizza_steak/train\"\n",
        "test_dir  = \"pizza_steak/test\"\n",
        "\n",
        "# Import data from directories and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224,224),\n",
        "                                               class_mode=\"binary\",\n",
        "                                               seed=42)\n",
        "\n",
        "valid_data = valid_datagen.flow_from_directory(test_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224,224),\n",
        "                                               class_mode=\"binary\",\n",
        "                                               seed=42)\n",
        "\n",
        "\n",
        "# Craete a CNN Model (Same as tiny VGG -https://poloclub.github.io/cnn-explainer/ )\n",
        "\n",
        "model_1 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters=10,\n",
        "                           kernel_size=3,\n",
        "                           activation=\"relu\",\n",
        "                           input_shape=(224,224,3)),\n",
        "    tf.keras.layers.Conv2D(10,3, activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(pool_size=2,padding=\"valid\"),\n",
        "    tf.keras.layers.Conv2D(10,3,activation=\"relu\"),\n",
        "    tf.keras.layers.Conv2D(10,3,activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(1,activation= \"sigmoid\")\n",
        "])\n",
        "\n",
        "# Compile the Model\n",
        "model_1.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "# Fit the Model\n",
        "history_1 = model_1.fit(train_data,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=valid_data,\n",
        "                        validation_steps=len(valid_data))\n",
        "\n"
      ],
      "metadata": {
        "id": "JXZHFlvHEcso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "9C-mkv-wLc_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create a model to replicate the TensorFlow Playground model\n",
        "model_2 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_2.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history_2 = model_2.fit(train_data, # use same training data created above\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=valid_data, # use same validation data created above\n",
        "                        validation_steps=len(valid_data))"
      ],
      "metadata": {
        "id": "eNQC5_uSVubP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model similar to model_1 but add an extra layer and increase the number of hidden units in each layer\n",
        "model_3 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input\n",
        "  tf.keras.layers.Dense(100, activation='relu'), # increase number of neurons from 4 to 100 (for each layer)\n",
        "  tf.keras.layers.Dense(100, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='relu'), # add an extra layer\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_3.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history_3 = model_3.fit(train_data,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=valid_data,\n",
        "                        validation_steps=len(valid_data))"
      ],
      "metadata": {
        "id": "nwNU8jCjYlPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out model_3 architecture\n",
        "model_3.summary()"
      ],
      "metadata": {
        "id": "i-ZzcaUHgPM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NR4lJm9-gSVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the create of our model little easier\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation\n",
        "from tensorflow.keras import Sequential\n",
        "\n"
      ],
      "metadata": {
        "id": "NSDCNqpKgd4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Model\n",
        "model_4 = Sequential([\n",
        "    Conv2D(filters=10,\n",
        "           kernel_size=3,\n",
        "           strides=1,\n",
        "           padding='valid',\n",
        "           activation=\"relu\",\n",
        "           input_shape=(224,224,3)),\n",
        "    Conv2D(10,3, activation='relu'),\n",
        "    Conv2D(10,3, activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(1,activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "NM6SmvEvhsRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.src.optimizers import optimizer\n",
        "# Compile the Model\n",
        "model_4.compile(loss='binary_crossentropy',\n",
        "                optimizer=Adam(),\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "NrWPBzL9ipcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binary classification: Let's break it down\n",
        "1. Become one with the data (visualize, visualize, visualize...)\n",
        "2. Preprocess the data (prepare it for a model)\n",
        "3. Create a model (start with a baseline)\n",
        "4. Fit the model\n",
        "5. Evaluate the model\n",
        "6. Adjust different parameters and improve model (try to beat your baseline)\n",
        "7. Repeat until satisfied"
      ],
      "metadata": {
        "id": "CABifPi1tofl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize data (requires function 'view_random_image' above)\n",
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "steak_img = view_random_image(\"pizza_steak/train/\", \"steak\")\n",
        "plt.subplot(1, 2, 2)\n",
        "pizza_img = view_random_image(\"pizza_steak/train/\", \"pizza\")"
      ],
      "metadata": {
        "id": "yRO1T1KAt4ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-process Data (Prepare it for the Model)"
      ],
      "metadata": {
        "id": "-LvfEF0AuGCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training and test directory paths\n",
        "train_dir = \"pizza_steak/train/\"\n",
        "test_dir  = \"pizza_steak/test/\""
      ],
      "metadata": {
        "id": "l3wBubd1uMgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create train and test data generators and rescale the data\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen  = ImageDataGenerator(rescale=1/255.)"
      ],
      "metadata": {
        "id": "KnoP5rraunWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(directory=train_dir,\n",
        "                                               target_size=(224,224),\n",
        "                                               class_mode='binary',\n",
        "                                               batch_size=32)\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(directory=test_dir,\n",
        "                                             target_size=(224,224),\n",
        "                                             class_mode='binary',\n",
        "                                             batch_size=32)\n"
      ],
      "metadata": {
        "id": "RcaTgFQbvUNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a sample of the training data batch\n",
        "images, labels = train_data.next()\n",
        "len(images),len(labels)"
      ],
      "metadata": {
        "id": "Quq-_1oiwtkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 1st two images\n",
        "images[:2], images[0].shape"
      ],
      "metadata": {
        "id": "pS2I0lLCw-aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#View the first natch of labels\n",
        "labels"
      ],
      "metadata": {
        "id": "p0zUgs5nx8za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the create of our model little easier\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation\n",
        "from tensorflow.keras import Sequential"
      ],
      "metadata": {
        "id": "RxEfUBWryHM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Model\n",
        "model_4 = Sequential([\n",
        "    Conv2D(filters=10,\n",
        "           kernel_size=3,\n",
        "           strides=1,\n",
        "           padding='valid',\n",
        "           activation=\"relu\",\n",
        "           input_shape=(224,224,3)),\n",
        "    Conv2D(10,3, activation='relu'),\n",
        "    Conv2D(10,3, activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(1,activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "-tg6kt2-yQWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model_4.compile(loss = 'binary_crossentropy',\n",
        "                optimizer=Adam(),\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "lprt6isayV3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checl length of training and test data generators\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "wlcjduJB0lTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history_4 = model_4.fit(train_data,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=test_data,\n",
        "                        validation_steps=len(test_data))"
      ],
      "metadata": {
        "id": "jTEtsTiN4YlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the training curves\n",
        "import pandas as pd\n",
        "pd.DataFrame(history_4.history).plot(figsize=(10,7))"
      ],
      "metadata": {
        "id": "No_vLCPm45YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the validation and training data separately\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "  \"\"\"\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();"
      ],
      "metadata": {
        "id": "lYmj2xiW5jD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the loss curves of model_4\n",
        "plot_loss_curves(history_4)"
      ],
      "metadata": {
        "id": "VS6mqlZJ5-aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out Model's architecture\n",
        "model_4.summary()"
      ],
      "metadata": {
        "id": "JE2eVH8o6BVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model ( a 3 layer Convolutional Neural Network)\n",
        "model_5 = Sequential([\n",
        "    Conv2D(10,3, activation='relu', input_shape=(224,224,3)),\n",
        "    MaxPool2D(pool_size=2),\n",
        "    Conv2D(10,3,activation='relu'),\n",
        "    MaxPool2D(),\n",
        "    Conv2D(10,3, activation='relu'),\n",
        "    MaxPool2D(),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "YeVflzO36dr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile Model\n",
        "model_5.compile(loss='binary_crossentropy',\n",
        "                optimizer=Adam(),\n",
        "                metrics='accuracy')\n"
      ],
      "metadata": {
        "id": "rt5lseNP9yw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Model\n",
        "history_5 = model_5.fit(train_data,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=test_data,\n",
        "                        validation_steps=len(test_data))"
      ],
      "metadata": {
        "id": "SLbgy8XtCGbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Model architecture\n",
        "model_5.summary()"
      ],
      "metadata": {
        "id": "i9zH0O7ACeTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss curves of model_5 results\n",
        "plot_loss_curves(history_5)"
      ],
      "metadata": {
        "id": "8MZjcNuLCvyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ImageDataGenerator training instance with data augmentation\n",
        "train_datagen_augmented = ImageDataGenerator(rescale=1/255.,\n",
        "                                             rotation_range=20, # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n",
        "                                             shear_range=0.2, # shear the image\n",
        "                                             zoom_range=0.2, # zoom into the image\n",
        "                                             width_shift_range=0.2, # shift the image width ways\n",
        "                                             height_shift_range=0.2, # shift the image height ways\n",
        "                                             horizontal_flip=True) # flip the image on the horizontal axis\n",
        "\n",
        "# Create ImageDataGenerator training instance without data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "# Create ImageDataGenerator test instance without data augmentation\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)"
      ],
      "metadata": {
        "id": "-0yVrqsVDwKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data and augment it from training directory\n",
        "print(\"Augmented training images:\")\n",
        "train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n",
        "                                                                   target_size=(224, 224),\n",
        "                                                                   batch_size=32,\n",
        "                                                                   class_mode='binary',\n",
        "                                                                   shuffle=False) # Don't shuffle for demonstration purposes, usually a good thing to shuffle\n",
        "\n",
        "# Create non-augmented data batches\n",
        "print(\"Non-augmented training images:\")\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               target_size=(224, 224),\n",
        "                                               batch_size=32,\n",
        "                                               class_mode='binary',\n",
        "                                               shuffle=False) # Don't shuffle for demonstration purposes\n",
        "\n",
        "print(\"Unchanged test images:\")\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=(224, 224),\n",
        "                                             batch_size=32,\n",
        "                                             class_mode='binary')"
      ],
      "metadata": {
        "id": "F_stuY0LF6_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the batch samples\n",
        "images,labels = train_data.next()\n",
        "augmented_images, augmented_labels = train_data_augmented.next()"
      ],
      "metadata": {
        "id": "x_uqX7ijHRKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show original image and augmented image\n",
        "random_number = random.randint(0,31)\n",
        "plt.imshow(images[random_number])\n",
        "plt.title(f\"Original Image\")\n",
        "plt.axis(False)\n",
        "plt.figure()\n",
        "\n",
        "plt.imshow(augmented_images[random_number])\n",
        "plt.title(f\"Augmented Image\")\n",
        "plt.axis(False)\n",
        "plt.figure()"
      ],
      "metadata": {
        "id": "_3DdB_P_Hpii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model (same as model_5)\n",
        "model_6 = Sequential([\n",
        "  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n",
        "  MaxPool2D(pool_size=2), # reduce number of features by half\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_6.compile(loss='binary_crossentropy',\n",
        "                optimizer=Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history_6 = model_6.fit(train_data_augmented, # changed to augmented training data\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data_augmented),\n",
        "                        validation_data=test_data,\n",
        "                        validation_steps=len(test_data))"
      ],
      "metadata": {
        "id": "YgLM5QFqIrWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check model's performance history training on augmented data\n",
        "plot_loss_curves(history_6)"
      ],
      "metadata": {
        "id": "oknuLLkcJ7VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data and augment it from directories\n",
        "train_data_augmented_shuffled = train_datagen_augmented.flow_from_directory(train_dir,\n",
        "                                                                            target_size=(224, 224),\n",
        "                                                                            batch_size=32,\n",
        "                                                                            class_mode='binary',\n",
        "                                                                            shuffle=True) # Shuffle data (default)"
      ],
      "metadata": {
        "id": "fz9QAqB7J8fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6XAlu35uLMWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model (same as model_5 and model_6)\n",
        "model_7 = Sequential([\n",
        "  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_7.compile(loss='binary_crossentropy',\n",
        "                optimizer=Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history_7 = model_7.fit(train_data_augmented_shuffled, # now the augmented data is shuffled\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data_augmented_shuffled),\n",
        "                        validation_data=test_data,\n",
        "                        validation_steps=len(test_data))"
      ],
      "metadata": {
        "id": "2vpblcmRKbYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_7)"
      ],
      "metadata": {
        "id": "esgWpKpQLORP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CNN model (same as Tiny VGG but for binary classification - https://poloclub.github.io/cnn-explainer/ )\n",
        "model_8 = Sequential([\n",
        "  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)), # same input shape as our images\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_8.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history_8 = model_8.fit(train_data_augmented_shuffled,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data_augmented_shuffled),\n",
        "                        validation_data=test_data,\n",
        "                        validation_steps=len(test_data))"
      ],
      "metadata": {
        "id": "2dUMDoPFLQfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CNN model (same as Tiny VGG but for binary classification - https://poloclub.github.io/cnn-explainer/ )\n",
        "model_8 = Sequential([\n",
        "  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)), # same input shape as our images\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_8.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history_8 = model_8.fit(train_data_augmented_shuffled,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data_augmented_shuffled),\n",
        "                        validation_data=test_data,\n",
        "                        validation_steps=len(test_data))"
      ],
      "metadata": {
        "id": "m0lgFT52OZY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check model_1 architecture (same as model_8)\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "ONemc0LcO5S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check model_1 architecture (same as model_8)\n",
        "model_8.summary()"
      ],
      "metadata": {
        "id": "epgCJX7gO6cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_1)"
      ],
      "metadata": {
        "id": "1EsiKLEZO-20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_8)"
      ],
      "metadata": {
        "id": "RFbXbHBZPMgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Prediction with our trainied model"
      ],
      "metadata": {
        "id": "pe1ItlAuPrBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes we are working with\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "YXDgslrkPxk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View our example image\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg\n",
        "steak = mpimg.imread(\"03-steak.jpeg\")\n",
        "plt.imshow(steak)\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "95lxYPPOP5Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steak.shape"
      ],
      "metadata": {
        "id": "DWSbs5yQQAzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to import an image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename, img_shape=224):\n",
        "  \"\"\"\n",
        "  Reads an image from filename, turns it into a tensor\n",
        "  and reshapes it to (img_shape, img_shape, colour_channel).\n",
        "  \"\"\"\n",
        "  # Read in target file (an image)\n",
        "  img = tf.io.read_file(filename)\n",
        "\n",
        "  # Decode the read file into a tensor & ensure 3 colour channels\n",
        "  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "\n",
        "  # Resize the image (to the same size our model was trained on)\n",
        "  img = tf.image.resize(img, size = [img_shape, img_shape])\n",
        "\n",
        "  # Rescale the image (get all values between 0 and 1)\n",
        "  img = img/255.\n",
        "  return img"
      ],
      "metadata": {
        "id": "Q7Vwd4kAQGpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steak = load_and_prep_image(\"03-steak.jpeg\")\n",
        "steak"
      ],
      "metadata": {
        "id": "8IGwgRpJQVRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction on our custom image\n",
        "model_8.predict(steak)"
      ],
      "metadata": {
        "id": "Wn1jHddAQoMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an extra axis\n",
        "print(f\"Shape before new dimension: {steak.shape}\")\n",
        "steak = tf.expand_dims(steak, axis=0) # add an extra dimension at axis 0\n",
        "#steak = steak[tf.newaxis, ...] # alternative to the above, '...' is short for 'every other dimension'\n",
        "print(f\"Shape after new dimension: {steak.shape}\")\n",
        "steak"
      ],
      "metadata": {
        "id": "1fqxVxqeQ0-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_8.predict(steak)"
      ],
      "metadata": {
        "id": "ZXSxjfZ9RGNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_and_plot(model, filename, class_names):\n",
        "  \"\"\"\n",
        "  Imports an image located at filename, makes a prediction on it with\n",
        "  a trained model and plots the image with the predicted class as the title.\n",
        "  \"\"\"\n",
        "  # Import the target image and preprocess it\n",
        "  img = load_and_prep_image(filename)\n",
        "\n",
        "  # Make a prediction\n",
        "  pred = model.predict(tf.expand_dims(img, axis=0))\n",
        "\n",
        "  # Get the predicted class\n",
        "  pred_class = class_names[int(tf.round(pred)[0][0])]\n",
        "\n",
        "  # Plot the image and predicted class\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Prediction: {pred_class}\")\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "ozaLjY9zRL6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our model on a custom image\n",
        "pred_and_plot(model_8, \"03-steak.jpeg\", class_names)"
      ],
      "metadata": {
        "id": "f8nPjDWTRz4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-class Classification"
      ],
      "metadata": {
        "id": "-aAD_sBQTsjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Download zip file of 10_food_classes images\n",
        "# See how this data was created - https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
        "\n",
        "# Unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"10_food_classes_all_data.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "-mr58trZTvni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Walk through 10_food_classes directory and list number of files\n",
        "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_all_data\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "r4JNwQyiTzlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"10_food_classes_all_data/train/\"\n",
        "test_dir = \"10_food_classes_all_data/test/\""
      ],
      "metadata": {
        "id": "aigSn-kvUKpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class names for our multi-class dataset\n",
        "import pathlib\n",
        "import numpy as np\n",
        "data_dir = pathlib.Path(train_dir)\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "9kpk1C8iUYOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View a random image from the training dataset\n",
        "import random\n",
        "img = view_random_image(target_dir=train_dir,\n",
        "                        target_class=random.choice(class_names)) # get a random class name"
      ],
      "metadata": {
        "id": "sEz8yRAgUbX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-process the data"
      ],
      "metadata": {
        "id": "MUkk6JayUm8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Rescale the data and create data generator instances\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "# Load data in from directories and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               target_size=(224, 224),\n",
        "                                               batch_size=32,\n",
        "                                               class_mode='categorical') # changed to categorical\n",
        "\n",
        "test_data = train_datagen.flow_from_directory(test_dir,\n",
        "                                              target_size=(224, 224),\n",
        "                                              batch_size=32,\n",
        "                                              class_mode='categorical')"
      ],
      "metadata": {
        "id": "y9cd3uNoUgkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJgeQvuWVB4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Model for Multi-class classification\n",
        "\n",
        "           \n"
      ],
      "metadata": {
        "id": "_oDeTwmEBWcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
        "\n",
        "# Create our Model\n",
        "model_9 = Sequential([\n",
        "  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(10, activation='softmax') # changed to have 10 neurons (same as number of classes) and 'softmax' activation\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_9.compile(loss=\"categorical_crossentropy\", # changed to categorical_crossentropy\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "5U6AGsCiCZmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "history_9 = model_9.fit(train_data, # now 10 different classes\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=test_data,\n",
        "                        validation_steps=len(test_data))"
      ],
      "metadata": {
        "id": "50v7U_WoCh4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model_9.evaluate(test_data)"
      ],
      "metadata": {
        "id": "G_WbwCdcC0Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(history_9)"
      ],
      "metadata": {
        "id": "WYTfuGHGKdTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJ34k7DHKnzj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}